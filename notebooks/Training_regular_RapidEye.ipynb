{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used to train the RapidEye regular models \n",
    "\n",
    "The models were trained in Google Colaboratory Virtual Environment, thus, to work properly, this notebook should be loaded in google drive.\n",
    "\n",
    "* [32x32 models](#32-x-32-models) \n",
    "\n",
    "* [64x64 models](#64-x-64-models) \n",
    "\n",
    "* [128x128 models](#128-x-128-models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as  tf\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install segmetation_models library (https://github.com/qubvel/segmentation_models)\n",
    "pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segmentation)models library \n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32 x 32 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data 32x32 - regular - the Strings are the directions to the .npy files in google drive\n",
    "X_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/32_32/regular/arrays/X_train_32_regular.npy\")\n",
    "Y_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/32_32/regular/arrays/Y_train_32_regular.npy\")\n",
    "\n",
    "# Load test data - Area 1\n",
    "X_test_area_1 = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/test/test_without_terrain/area_1/arrays/X_test_test_area_1.npy\")\n",
    "Y_test_area_1 = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/test/test_without_terrain/area_1/arrays/Y_test_test_area_1.npy\")\n",
    "\n",
    "# Load test data - Area 2\n",
    "X_test_area_2 = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/test/test_without_terrain/area_2/arrays/X_test_test_area_2.npy\")\n",
    "Y_test_area_2 = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/test/test_without_terrain/area_2/arrays/Y_test_test_area_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data dimensions\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\\nX_test_area_1 shape: {X_test_area_1.shape}, Y_test_area_1 shape: {Y_test_area_1.shape},\\nX_test_area_2 shape: {X_test_area_2.shape}, Y_test_area_2 shape: {Y_test_area_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics - Precision, Recall, FScore, IoU\n",
    "metrics = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5),sm.metrics.FScore(threshold=0.5,beta=1),sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet Architecture\n",
    "def Unet_Original(lr,filtersFirstLayer, pretrained_weights = None,input_size = (32,32,5)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(inputs)\n",
    "    conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool1)\n",
    "    conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool2)\n",
    "    conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool3)\n",
    "    conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool4)\n",
    "    conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv5)\n",
    "\n",
    "    up6 = Conv2D(filtersFirstLayer*8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv5))\n",
    "    merge6 = concatenate([conv4,up6], axis = 3)\n",
    "    conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge6)\n",
    "    conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(filtersFirstLayer*4, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge7)\n",
    "    conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(filtersFirstLayer*2, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge8)\n",
    "    conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(filtersFirstLayer, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge9)\n",
    "    conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = lr), loss = 'binary_crossentropy', metrics = metrics)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training - Results are saved in a .csv file\n",
    "\n",
    "# size of the tiles\n",
    "size = 32\n",
    "# Sampling method\n",
    "sampling = \"regular\"\n",
    "# number of filters \n",
    "filters = [16,32,64]\n",
    "# lr = 0.001\n",
    "lr = [10e-4]\n",
    "# batch sizes \n",
    "batch_size = [16,32,64,128]\n",
    "\n",
    "# dictionary that will save the results\n",
    "dic = {}\n",
    "\n",
    "# Hyperparameters\n",
    "dic[\"model\"] = []\n",
    "dic[\"batch_size\"] = []\n",
    "dic[\"learning_rate\"] = []\n",
    "dic[\"filters\"] = []\n",
    "\n",
    "# test area 1\n",
    "dic[\"precision_area_1\"] = []\n",
    "dic[\"recall_area_1\"] = []\n",
    "dic[\"f1_score_area_1\"] = []\n",
    "dic[\"iou_score_area_1\"] = []\n",
    "\n",
    "# test area 2\n",
    "dic[\"precision_area_2\"] = []\n",
    "dic[\"recall_area_2\"] = []\n",
    "dic[\"f1_score_area_2\"] = []\n",
    "dic[\"iou_score_area_2\"] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop over all the filters in the filter list\n",
    "for fiilter in filters:\n",
    "    # loop over the learning rates (used to evalute 0.01 and 0.0001 without good results)\n",
    "    for learning_rate in lr:\n",
    "        # loop over all batch sizes in batch_size list\n",
    "        for batch in batch_size:\n",
    "            # load the model\n",
    "            model = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate)\n",
    "            # Save the models only when validation loss decrease\n",
    "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5', monitor='val_loss', mode='min',verbose=1, save_best_only=True,save_weights_only = True)\n",
    "            # Stop after 20 epochs without decreasing the validation loss\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "            print(fiilter, learning_rate,batch)\n",
    "            # fit the model 30% of the dataset was used as validation\n",
    "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=200,validation_split=0.3,callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "            # summarize history for iou score\n",
    "            plt.plot(history.history['iou_score'])\n",
    "            plt.plot(history.history['val_iou_score'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            # save plots \n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            # load unet to evaluate the test data\n",
    "            unet_original = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(1024,1024,5))\n",
    "            # load the last saved weight from the training\n",
    "            unet_original.load_weights(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5\")\n",
    "            \n",
    "           # Evaluate test area 1\n",
    "            res_1 = unet_original.evaluate(X_test_area_1,Y_test_area_1)\n",
    "            \n",
    "            # Evaluate test area 2\n",
    "            res_2 = unet_original.evaluate(X_test_area_2,Y_test_area_2)\n",
    "\n",
    "            # Data to plot the predicted output\n",
    "            preds_train_1 = unet_original.predict(X_test_area_1, verbose=1)\n",
    "            preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
    "            preds_train_2 = unet_original.predict(X_test_area_2, verbose=1)\n",
    "            preds_train_t2 = (preds_train_2 > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "            # save results on the dictionary\n",
    "            dic[\"model\"].append(\"Unet\")\n",
    "            dic[\"batch_size\"].append(batch)\n",
    "            dic[\"learning_rate\"].append(learning_rate)\n",
    "            dic[\"filters\"].append(fiilter)\n",
    "            dic[\"precision_area_1\"].append(res_1[1])\n",
    "            dic[\"recall_area_1\"].append(res_1[2])\n",
    "            dic[\"f1_score_area_1\"].append(res_1[3])\n",
    "            dic[\"iou_score_area_1\"].append(res_1[4])\n",
    "           \n",
    "            dic[\"precision_area_2\"].append(res_2[1])\n",
    "            dic[\"recall_area_2\"].append(res_2[2])\n",
    "            dic[\"f1_score_area_2\"].append(res_2[3])\n",
    "            dic[\"iou_score_area_2\"].append(res_2[4])\n",
    "            \n",
    "    \n",
    "            # Plot the results and save the plots\n",
    "            f, axarr = plt.subplots(2,3,figsize=(10,10))\n",
    "            axarr[0,0].imshow(X_test_area_1[0][:,:,:3])\n",
    "            axarr[0,1].imshow(np.squeeze(preds_train_t1[0]))\n",
    "            axarr[0,2].imshow(np.squeeze(Y_test_area_1[0]))\n",
    "            axarr[1,0].imshow(X_test_area_2[0][:,:,:3])\n",
    "            axarr[1,1].imshow(np.squeeze(preds_train_t2[0]))\n",
    "            axarr[1,2].imshow(np.squeeze(Y_test_area_2[0]))\n",
    "            f.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/images/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_result.png\")\n",
    "      \n",
    "            # Convert results to a dataframe\n",
    "            results = pd.DataFrame(dic)\n",
    "            # Export as csv\n",
    "            results.to_csv(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/result_table/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 64 x 64 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data 32x32 - regular - the Strings are the directions to the .npy files in google drive\n",
    "X_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/64_64/regular/arrays/X_train_64_regular.npy\")\n",
    "Y_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/64_64/regular/arrays/Y_train_64_regular.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data dimensions\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\\nX_test_area_1 shape: {X_test_area_1.shape}, Y_test_area_1 shape: {Y_test_area_1.shape},\\nX_test_area_2 shape: {X_test_area_2.shape}, Y_test_area_2 shape: {Y_test_area_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training - Results are saved in a .csv file\n",
    "\n",
    "# size of the tiles\n",
    "size = 64\n",
    "# Sampling method\n",
    "sampling = \"regular\"\n",
    "# number of filters \n",
    "filters = [16,32,64]\n",
    "# lr = 0.001\n",
    "lr = [10e-4]\n",
    "# batch sizes \n",
    "batch_size = [16,32,64,128]\n",
    "\n",
    "# dictionary that will save the results\n",
    "dic = {}\n",
    "\n",
    "# Hyperparameters\n",
    "dic[\"model\"] = []\n",
    "dic[\"batch_size\"] = []\n",
    "dic[\"learning_rate\"] = []\n",
    "dic[\"filters\"] = []\n",
    "\n",
    "# test area 1\n",
    "dic[\"precision_area_1\"] = []\n",
    "dic[\"recall_area_1\"] = []\n",
    "dic[\"f1_score_area_1\"] = []\n",
    "dic[\"iou_score_area_1\"] = []\n",
    "\n",
    "# test area 2\n",
    "dic[\"precision_area_2\"] = []\n",
    "dic[\"recall_area_2\"] = []\n",
    "dic[\"f1_score_area_2\"] = []\n",
    "dic[\"iou_score_area_2\"] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop over all the filters in the filter list\n",
    "for fiilter in filters:\n",
    "    # loop over the learning rates (used to evalute 0.01 and 0.0001 without good results)\n",
    "    for learning_rate in lr:\n",
    "        # loop over all batch sizes in batch_size list\n",
    "        for batch in batch_size:\n",
    "            # load the model\n",
    "            model = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate, input_size = (64,64,5))\n",
    "            # Save the models only when validation loss decrease\n",
    "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5', monitor='val_loss', mode='min',verbose=1, save_best_only=True,save_weights_only = True)\n",
    "            # Stop after 20 epochs without decreasing the validation loss\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "            print(fiilter, learning_rate,batch)\n",
    "            # fit the model 30% of the dataset was used as validation\n",
    "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=200,validation_split=0.3,callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "            # summarize history for iou score\n",
    "            plt.plot(history.history['iou_score'])\n",
    "            plt.plot(history.history['val_iou_score'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            # save plots \n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            # load unet to evaluate the test data\n",
    "            unet_original = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(1024,1024,5))\n",
    "            # load the last saved weight from the training\n",
    "            unet_original.load_weights(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5\")\n",
    "            \n",
    "           # Evaluate test area 1\n",
    "            res_1 = unet_original.evaluate(X_test_area_1,Y_test_area_1)\n",
    "            \n",
    "            # Evaluate test area 2\n",
    "            res_2 = unet_original.evaluate(X_test_area_2,Y_test_area_2)\n",
    "\n",
    "            # Data to plot the predicted output\n",
    "            preds_train_1 = unet_original.predict(X_test_area_1, verbose=1)\n",
    "            preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
    "            preds_train_2 = unet_original.predict(X_test_area_2, verbose=1)\n",
    "            preds_train_t2 = (preds_train_2 > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "            # save results on the dictionary\n",
    "            dic[\"model\"].append(\"Unet\")\n",
    "            dic[\"batch_size\"].append(batch)\n",
    "            dic[\"learning_rate\"].append(learning_rate)\n",
    "            dic[\"filters\"].append(fiilter)\n",
    "            dic[\"precision_area_1\"].append(res_1[1])\n",
    "            dic[\"recall_area_1\"].append(res_1[2])\n",
    "            dic[\"f1_score_area_1\"].append(res_1[3])\n",
    "            dic[\"iou_score_area_1\"].append(res_1[4])\n",
    "           \n",
    "            dic[\"precision_area_2\"].append(res_2[1])\n",
    "            dic[\"recall_area_2\"].append(res_2[2])\n",
    "            dic[\"f1_score_area_2\"].append(res_2[3])\n",
    "            dic[\"iou_score_area_2\"].append(res_2[4])\n",
    "            \n",
    "    \n",
    "            # Plot the results and save the plots\n",
    "            f, axarr = plt.subplots(2,3,figsize=(10,10))\n",
    "            axarr[0,0].imshow(X_test_area_1[0][:,:,:3])\n",
    "            axarr[0,1].imshow(np.squeeze(preds_train_t1[0]))\n",
    "            axarr[0,2].imshow(np.squeeze(Y_test_area_1[0]))\n",
    "            axarr[1,0].imshow(X_test_area_2[0][:,:,:3])\n",
    "            axarr[1,1].imshow(np.squeeze(preds_train_t2[0]))\n",
    "            axarr[1,2].imshow(np.squeeze(Y_test_area_2[0]))\n",
    "            f.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/images/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_result.png\")\n",
    "      \n",
    "            # Convert results to a dataframe\n",
    "            results = pd.DataFrame(dic)\n",
    "            # Export as csv\n",
    "            results.to_csv(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/result_table/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 128 x 128 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data 32x32 - regular - the Strings are the directions to the .npy files in google drive\n",
    "X_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/128_128/regular/arrays/X_train_128_regular.npy\")\n",
    "Y_train = np.load(\"/content/drive/My Drive/Mestrado/artigo/artigo_final/data/non_augmented/128_128/regular/arrays/Y_train_128_regular.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data dimensions\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\\nX_test_area_1 shape: {X_test_area_1.shape}, Y_test_area_1 shape: {Y_test_area_1.shape},\\nX_test_area_2 shape: {X_test_area_2.shape}, Y_test_area_2 shape: {Y_test_area_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training - Results are saved in a .csv file\n",
    "\n",
    "# size of the tiles\n",
    "size = 128\n",
    "# Sampling method\n",
    "sampling = \"regular\"\n",
    "# number of filters \n",
    "filters = [16,32,64]\n",
    "# lr = 0.001\n",
    "lr = [10e-4]\n",
    "# batch sizes \n",
    "batch_size = [16,32,64,128]\n",
    "\n",
    "# dictionary that will save the results\n",
    "dic = {}\n",
    "\n",
    "# Hyperparameters\n",
    "dic[\"model\"] = []\n",
    "dic[\"batch_size\"] = []\n",
    "dic[\"learning_rate\"] = []\n",
    "dic[\"filters\"] = []\n",
    "\n",
    "# test area 1\n",
    "dic[\"precision_area_1\"] = []\n",
    "dic[\"recall_area_1\"] = []\n",
    "dic[\"f1_score_area_1\"] = []\n",
    "dic[\"iou_score_area_1\"] = []\n",
    "\n",
    "# test area 2\n",
    "dic[\"precision_area_2\"] = []\n",
    "dic[\"recall_area_2\"] = []\n",
    "dic[\"f1_score_area_2\"] = []\n",
    "dic[\"iou_score_area_2\"] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop over all the filters in the filter list\n",
    "for fiilter in filters:\n",
    "    # loop over the learning rates (used to evalute 0.01 and 0.0001 without good results)\n",
    "    for learning_rate in lr:\n",
    "        # loop over all batch sizes in batch_size list\n",
    "        for batch in batch_size:\n",
    "            # load the model\n",
    "            model = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate, input_size = (128,128,5))\n",
    "            # Save the models only when validation loss decrease\n",
    "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5', monitor='val_loss', mode='min',verbose=1, save_best_only=True,save_weights_only = True)\n",
    "            # Stop after 20 epochs without decreasing the validation loss\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "            print(fiilter, learning_rate,batch)\n",
    "            # fit the model 30% of the dataset was used as validation\n",
    "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=200,validation_split=0.3,callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "            # summarize history for iou score\n",
    "            plt.plot(history.history['iou_score'])\n",
    "            plt.plot(history.history['val_iou_score'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            # save plots \n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/plots/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            # load unet to evaluate the test data\n",
    "            unet_original = Unet_Original(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(1024,1024,5))\n",
    "            # load the last saved weight from the training\n",
    "            unet_original.load_weights(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/model/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5\")\n",
    "            \n",
    "           # Evaluate test area 1\n",
    "            res_1 = unet_original.evaluate(X_test_area_1,Y_test_area_1)\n",
    "            \n",
    "            # Evaluate test area 2\n",
    "            res_2 = unet_original.evaluate(X_test_area_2,Y_test_area_2)\n",
    "\n",
    "            # Data to plot the predicted output\n",
    "            preds_train_1 = unet_original.predict(X_test_area_1, verbose=1)\n",
    "            preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
    "            preds_train_2 = unet_original.predict(X_test_area_2, verbose=1)\n",
    "            preds_train_t2 = (preds_train_2 > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "            # save results on the dictionary\n",
    "            dic[\"model\"].append(\"Unet\")\n",
    "            dic[\"batch_size\"].append(batch)\n",
    "            dic[\"learning_rate\"].append(learning_rate)\n",
    "            dic[\"filters\"].append(fiilter)\n",
    "            dic[\"precision_area_1\"].append(res_1[1])\n",
    "            dic[\"recall_area_1\"].append(res_1[2])\n",
    "            dic[\"f1_score_area_1\"].append(res_1[3])\n",
    "            dic[\"iou_score_area_1\"].append(res_1[4])\n",
    "           \n",
    "            dic[\"precision_area_2\"].append(res_2[1])\n",
    "            dic[\"recall_area_2\"].append(res_2[2])\n",
    "            dic[\"f1_score_area_2\"].append(res_2[3])\n",
    "            dic[\"iou_score_area_2\"].append(res_2[4])\n",
    "            \n",
    "    \n",
    "            # Plot the results and save the plots\n",
    "            f, axarr = plt.subplots(2,3,figsize=(10,10))\n",
    "            axarr[0,0].imshow(X_test_area_1[0][:,:,:3])\n",
    "            axarr[0,1].imshow(np.squeeze(preds_train_t1[0]))\n",
    "            axarr[0,2].imshow(np.squeeze(Y_test_area_1[0]))\n",
    "            axarr[1,0].imshow(X_test_area_2[0][:,:,:3])\n",
    "            axarr[1,1].imshow(np.squeeze(preds_train_t2[0]))\n",
    "            axarr[1,2].imshow(np.squeeze(Y_test_area_2[0]))\n",
    "            f.savefig(f\"/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/images/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_result.png\")\n",
    "      \n",
    "            # Convert results to a dataframe\n",
    "            results = pd.DataFrame(dic)\n",
    "            # Export as csv\n",
    "            results.to_csv(f'/content/drive/My Drive/Mestrado/artigo/artigo_final/results/non_augmented/{size}_{size}/{sampling}/result_table/unet/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
